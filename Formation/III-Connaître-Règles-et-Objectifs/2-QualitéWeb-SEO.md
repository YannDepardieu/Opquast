# Qualité Web et SEO

## Quelques termes clés :

- Contenu non textuel
- Exigences génériques
- Lien entrant
- SEO
- VPTCS

## Transcription de la vidéo

Il y a quelques années, nous avons été invités à donner une conférence dans le cadre du SEO campus à Lille. Cette conférence s’intitulait « Mégalomanie et qualité Web, tout ce que j’ai à apprendre à Google ». Comme votre temps est compté mais que ce qui était dit est important, nous avons décidé de vous faire un petit résumé. C’est parti.
Google vend de la publicité, certes, mais à part ça quel est son métier ?
Depuis l’origine, Google est d’abord un moteur de recherche. Mais son métier, ce n’est pas seulement d’indexer des sites. Indexer des sites, récupérer du contenu, c’est relativement simple. Son métier, c’est de faire du classement, du ranking, en anglais.
Alors, classer des sites, c’est très compliqué. Comment classer des sites ?

Dans un premier temps, Google a révolutionné le classement de pages web en privilégiant la mesure de popularité – le nombre de liens qui pointent vers un site – plutôt qu’en s’appuyant uniquement sur la pertinence du résultat. C’est le fameux algorithme pagerank qui leur servait presque exclusivement au cours des premières années d’existence.
À cette époque, les recommandations Google proposaient à ce qu’on appelait encore les webmasters d’améliorer leur classement en augmentant le nombre de sites qui pointaient vers leurs pages (increasing the number of high-quality sites that link to their pages).

Avoir des liens entrants, c’est bien, mais même avec de nombreux liens de qualité vers votre contenu, ce n’est pas suffisant. Si votre site est lent, ne respecte pas les standards, n’est pas bien codé, ne décrit pas bien les contenus non textuels, les utilisateurs ne vont pas être satisfaits et les robots ne vont pas pouvoir faire correctement l’indexation. Sans compter que ce n’est toujours fiable : certaines techniques SEO permettent de créer de faux liens.
Bref, rapidement, ce mode de classement sur la popularité et le pagerank s’est avéré insuffisant. Google a dû s’intéresser à la qualité des sites en eux-mêmes et pas seulement à celle des liens qui pointaient vers eux.

En 2013, je suis tombé sur un lien signalant une évolution des règles Google. Ces règles annoncent que les webmasters peuvent améliorer leur classement non pas en augmentant le nombre de sites qui pointent vers leurs pages mais en créant des sites de grande qualité que les utilisateurs voudront utiliser et partager. Autrement dit : Google classe les sites en fonction de leur qualité. Il n’est plus question du travail des autres mais de votre travail, de votre site et de sa qualité.

Alors, finalement, lorsque l’on regarde ce qui s’est passé dans les 15 dernières années, Google s’est intéressé en permanence à la qualité des sites.
Il a intégré des critères de rapidité et de performance. Il a mobilisé les internautes pour évaluer les sites avec des boutons +1. Il a essayé d’obtenir des infos supplémentaires avec Google+. Il a fourni et étoffé les Webmasters Tools. Il a donné des consignes relatives à la qualité qui allaient dans le sens de l’optimisation, du respect des standards et de l’accessibilité. Il a fourni des outils d’évaluation comme Lighthouse. Avec un petit tag de rien du tout à ajouter sur les pages Web, il a proposé le service Google Analytics aux webmasters.

Cet outil d’analyse de fréquentation et de comportement des utilisateurs a transformé le SEO. Mais c’est promis, même si Google a un besoin vital d’avoir des infos sur la qualité de vos pages, ils nous ont dit qu’ils n’utiliseraient jamais les statistiques Google Analytics pour leur propre compte. Alors, on les croit parce ce sont des gens gentils qui ne font pas de mal, c’est ce qu’ils ont dit au début. Mais bref.

Quand les algorithmes, les outils et les statistiques ne suffisent plus, il faut faire appel à l’humain pour évaluer la confiance, l’autorité, la fiabilité etc. Il y a des humains qui font cela chez Google. Savez-vous comment on les appelle ? ce sont les Quality Raters.

Alors, le problème, c’est que la qualité, chacun en a sa propre définition. Alors je vous propose une définition qui est directement dérivée d’une définition internationale de ce qu’est la qualité : « La qualité Web, c’est l’aptitude d’un service en ligne à satisfaire des exigences implicites ou explicites ».
Logiquement, si Google veut classer les sites en fonction de leur qualité, il va essayer de la faire en fonction de leur aptitude à satisfaire des exigences implicites ou explicites. En conséquence, tout ce qui nuit à la qualité Web sera tôt ou tard sanctionné, parce que Google va continuer à améliorer ses algorithmes ; parce que son boulot, c’est de classer les sites en fonction de leur qualité. Pour prévenir les risques de déclassement, il vaut mieux respecter ces consignes et ces recommandations en matière de qualité.

La conférence est très modestement nommée « Mégalomanie et qualité Web, tout ce que j’ai à apprendre à Google » parce que chez Opquast, nous avons pris beaucoup d’avance sur Google, avec le modèle VPTCS et avec les check-lists qualité Web. Certaines règles de 2004 n’ont été prises en compte que beaucoup plus tard par Google. Certaines mauvaises pratiques identifiées dans Google ont provoqué des chutes de classement lors de certaines mise à jour de l’index.

Alors, voilà, lorsque vous vous formez à l’assurance qualité Web, vous faites du SEO. Inversement, le SEO de demain et même d’aujourd’hui, c’est peut-être tout simplement d’optimiser la qualité des sites.